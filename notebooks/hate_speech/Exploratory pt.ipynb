{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/paulafortuna/Portuguese-Hate-Speech-Dataset/master/2019-05-28_portuguese_hate_speech_hierarchical_classification.csv\"\n",
    "\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'Hate.speech', 'Sexism', 'Body', 'Racism', 'Ideology',\n",
       "       'Homophobia', 'Origin', 'Religion', 'Health', 'OtherLifestyle',\n",
       "       'Aborting.women', 'Agnostic', 'Argentines', 'Asians', 'Autists',\n",
       "       'Black.Women', 'Blond.women', 'Brazilians.women', 'Chinese',\n",
       "       'Criminals', 'Egyptians', 'Fat.people', 'Football.players.women',\n",
       "       'Gamers', 'Homeless', 'Homeless.women', 'Indigenous', 'Iranians',\n",
       "       'Japaneses', 'Jews', 'Jornalists', 'Latins', 'Left.wing.ideology',\n",
       "       'Men.Feminists', 'Mexicans', 'Muslims.women', 'Nordestines',\n",
       "       'Old.people', 'Polyamorous', 'Poor.people', 'Rural.people', 'Russians',\n",
       "       'Sertanejos', 'Street.artist', 'Ucranians', 'Vegetarians',\n",
       "       'White.people', 'Young.people', 'Old.women', 'Ugly.people',\n",
       "       'Venezuelans', 'Angolans', 'Black.people', 'Disabled.people',\n",
       "       'Fat.women', 'Feminists', 'Gays', 'Immigrants', 'Islamists', 'Lesbians',\n",
       "       'Men', 'Muslims', 'Refugees', 'Trans.women', 'Travestis', 'Women',\n",
       "       'Bissexuals', 'Transexuals', 'Ugly.women', 'Thin.women', 'Arabic',\n",
       "       'East.europeans', 'Africans', 'South.Americans', 'Brazilians',\n",
       "       'Migrants', 'Homossexuals', 'Thin.people', 'Ageing'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hate_speech_portuguese (/users/jmperez/.cache/huggingface/datasets/hate_speech_portuguese/default/1.0.0/20ad7529b5939c566862ef7d6753aa52f92c45aed182decf84abec62c7894062)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7399a100e6de4ae3af65d79572aad455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hate_speech_portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5668, 80)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3627, 80), (907, 80), (1134, 80))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split df into train, dev, test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = ['Hate.speech', 'Sexism', 'Body', 'Racism', 'Ideology',\n",
    "       'Homophobia']\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df, dev_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df.shape, dev_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hate.speech    1228\n",
       "Sexism          672\n",
       "Body            164\n",
       "Racism           94\n",
       "Ideology         92\n",
       "Homophobia      322\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[labels].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'Hate.speech', 'Sexism', 'Body', 'Racism', 'Ideology',\n",
       "       'Homophobia', 'Origin', 'Religion', 'Health', 'OtherLifestyle',\n",
       "       'Aborting.women', 'Agnostic', 'Argentines', 'Asians', 'Autists',\n",
       "       'Black.Women', 'Blond.women', 'Brazilians.women', 'Chinese',\n",
       "       'Criminals', 'Egyptians', 'Fat.people', 'Football.players.women',\n",
       "       'Gamers', 'Homeless', 'Homeless.women', 'Indigenous', 'Iranians',\n",
       "       'Japaneses', 'Jews', 'Jornalists', 'Latins', 'Left.wing.ideology',\n",
       "       'Men.Feminists', 'Mexicans', 'Muslims.women', 'Nordestines',\n",
       "       'Old.people', 'Polyamorous', 'Poor.people', 'Rural.people', 'Russians',\n",
       "       'Sertanejos', 'Street.artist', 'Ucranians', 'Vegetarians',\n",
       "       'White.people', 'Young.people', 'Old.women', 'Ugly.people',\n",
       "       'Venezuelans', 'Angolans', 'Black.people', 'Disabled.people',\n",
       "       'Fat.women', 'Feminists', 'Gays', 'Immigrants', 'Islamists', 'Lesbians',\n",
       "       'Men', 'Muslims', 'Refugees', 'Trans.women', 'Travestis', 'Women',\n",
       "       'Bissexuals', 'Transexuals', 'Ugly.women', 'Thin.women', 'Arabic',\n",
       "       'East.europeans', 'Africans', 'South.Americans', 'Brazilians',\n",
       "       'Migrants', 'Homossexuals', 'Thin.people', 'Ageing'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2162214/2237971902.py:12: FutureWarning: Passing a dict as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  train = Dataset.from_pandas(train_df[features], features=features, preserve_index=False)\n",
      "/tmp/ipykernel_2162214/2237971902.py:13: FutureWarning: Passing a dict as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  dev = Dataset.from_pandas(dev_df[features], features=features, preserve_index=False)\n",
      "/tmp/ipykernel_2162214/2237971902.py:14: FutureWarning: Passing a dict as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  test = Dataset.from_pandas(test_df[features], features=features, preserve_index=False)\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset, Value, Features, ClassLabel\n",
    "features = Features({\n",
    "    'text': Value('string'),\n",
    "    'Sexism': ClassLabel(num_classes=2, names=[\"non sexist\", \"sexist\"]),\n",
    "    'Body': ClassLabel(num_classes=2, names=[\"non body\", \"body\"]),\n",
    "    'Racism': ClassLabel(num_classes=2, names=[\"non racist\", \"racist\"]),\n",
    "    'Ideology': ClassLabel(num_classes=2, names=[\"non ideological\", \"ideological\"]),\n",
    "    'Homophobia': ClassLabel(num_classes=2, names=[\"non homophobic\", \"homophobic\"]),\n",
    "})\n",
    "\n",
    "\n",
    "train = Dataset.from_pandas(train_df[features], features=features, preserve_index=False)\n",
    "dev = Dataset.from_pandas(dev_df[features], features=features, preserve_index=False)\n",
    "test = Dataset.from_pandas(test_df[features], features=features, preserve_index=False)\n",
    "\n",
    "ds = DatasetDict(\n",
    "    train=train,\n",
    "    dev=dev,\n",
    "    test=test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29274cc823674a44b4a056ed333f4e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d421fe2d574bda9066b9736f920b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23de6cf22864f0e87c7158457462916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020a25f158db4928acf5e68772ff02fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split dev to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98b2bba2cf6499094489be2b7a90669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4a326368d3481dabbd1b457b353542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3605f89930bd4e588cb26cfc18ac64f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09b1f33f0914576ae0e9e20ffecc43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf399bb8aac4365ba7f3bccd029f02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b00276cab742a4b3d731fbbccf6a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e88f52a85bd445891a7b029b7b636b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bdd4b916f6488a921e3a34c8608ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.push_to_hub(\"pysentimiento/pt_hate_speech\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration pysentimiento--pt_hate_speech-0a3eb54f07f627a1\n",
      "Found cached dataset parquet (/users/jmperez/.cache/huggingface/datasets/pysentimiento___parquet/pysentimiento--pt_hate_speech-0a3eb54f07f627a1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054dba7aed4d4b2c95e9fc6fc8e79f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['text', 'Sexism', 'Body', 'Racism', 'Ideology', 'Homophobia'],\n",
       "        num_rows: 907\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'Sexism', 'Body', 'Racism', 'Ideology', 'Homophobia'],\n",
       "        num_rows: 1134\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'Sexism', 'Body', 'Racism', 'Ideology', 'Homophobia'],\n",
       "        num_rows: 3627\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"pysentimiento/pt_hate_speech\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc79ed69e4c2b5a1db8fa17ebb1e82d66421519e5b018d314116a7b4cda9238"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

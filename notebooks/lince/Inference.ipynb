{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference.ipynb  ner  ner_spa_eng.txt  pos  sentiment  submissions\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "model_path = \"..\"\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t test_results.json\ttraining_args.bin\n",
      "pytorch_model.bin\t tokenizer.json\n",
      "special_tokens_map.json  tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls ../../models/robertuito-lince-ner-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../models/robertuito-lince-ner-uncased\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def label_words(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns decoded labels\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "\n",
    "    model: AutoModelForTokenClassification\n",
    "        Labeling model\n",
    "    tokenizer: AutoTokenizer\n",
    "        Tokenizer\n",
    "    text: str or list of str\n",
    "        Text to be labeled\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "\n",
    "    word, labels: tuple of list of str\n",
    "        Words and their respective labels\n",
    "        labels are one of\n",
    "            \"O\",\n",
    "            \"B-marker\", \"I-marker\"\n",
    "            \"B-reference\", \"I-reference\"\n",
    "            \"B-term\", \"I-term\"\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    is_split_into_words = type(text) is list\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    word_ids = inputs.word_ids()\n",
    "\n",
    "    outputs = model(**inputs).logits\n",
    "    predictions = torch.argmax(outputs, dim=2).view(-1)\n",
    "\n",
    "    id2label = model.config.id2label\n",
    "\n",
    "    current_word_id = None\n",
    "    current_label = None\n",
    "\n",
    "    word_and_labels = []\n",
    "    for word_id, label in zip(word_ids, predictions):\n",
    "        label = label.item()\n",
    "        label_name = id2label[label]\n",
    "        if word_id != current_word_id:\n",
    "            # Starts new word\n",
    "            if current_word_id is not None:\n",
    "                if is_split_into_words:\n",
    "                    word = text[current_word_id]\n",
    "                else:\n",
    "                    word_span = inputs.word_to_chars(current_word_id)\n",
    "                    word = text[word_span[0]:word_span[1]]\n",
    "\n",
    "                word_and_labels.append((word.strip(), current_label))\n",
    "\n",
    "            current_label = label_name\n",
    "            current_word_id = word_id\n",
    "\n",
    "    if current_word_id:\n",
    "        word_and_labels.append((word, current_label))\n",
    "    return word_and_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('El', 'O'),\n",
       " ('presidente', 'O'),\n",
       " ('de', 'O'),\n",
       " ('la', 'O'),\n",
       " ('República', 'O'),\n",
       " ('es', 'O'),\n",
       " ('una', 'O'),\n",
       " ('persona', 'O')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_words(\"El presidente de la República es una persona\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('El', 'O'),\n",
       " ('presidente', 'O'),\n",
       " ('de', 'O'),\n",
       " ('la', 'O'),\n",
       " ('República', 'O'),\n",
       " ('es', 'O'),\n",
       " ('John', 'B-PER'),\n",
       " ('Wayne', 'I-PER'),\n",
       " ('y', 'O'),\n",
       " ('es', 'O'),\n",
       " ('dueño', 'O'),\n",
       " ('de', 'O'),\n",
       " ('GreenTugo', 'B-ORG')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_words(\"El presidente de la República es John Wayne y es dueño de GreenTugo\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Esto', 'O'), ('es', 'O'), ('Tugolandia', 'B-LOC')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_words(\"Esto es Tugolandia\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_segments(word_and_labels):\n",
    "    \"\"\"\n",
    "    Convert BIO labels to segments\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "\n",
    "    word_and_labels: list of tuple of (spacy.Token, str)\n",
    "        The word and label pairs.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "\n",
    "    segments: list of dicts\n",
    "        The segments.\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    current_words = None\n",
    "    current_type = None\n",
    "    for word, label in word_and_labels:\n",
    "        if label == 'O':\n",
    "            if current_words:\n",
    "                ret.append({\n",
    "                    \"tokens\": current_words,\n",
    "                    \"type\": current_type\n",
    "                })\n",
    "            current_type = None\n",
    "            current_words = None\n",
    "        elif label.startswith('B-'):\n",
    "            if current_words:\n",
    "                ret.append({\n",
    "                    \"tokens\": current_words,\n",
    "                    \"type\": current_type\n",
    "                })\n",
    "            current_words = [word]\n",
    "            current_type = label[2:]\n",
    "        elif label.startswith('I-'):\n",
    "            # If we are in the same type, add the word\n",
    "            if not current_words:\n",
    "                current_words = [word]\n",
    "                current_type = label[2:]\n",
    "            # Ignoring type... this could be a possible error\n",
    "            current_words.append(word)\n",
    "            \n",
    "\n",
    "    if current_words:\n",
    "        ret.append({\n",
    "            \"tokens\": current_words,\n",
    "            \"type\": current_type\n",
    "        })\n",
    "\n",
    "    for segment in ret:\n",
    "        segment[\"text\"] = \" \".join(segment[\"tokens\"])\n",
    "    return ret\n",
    "\n",
    "def detect_entities(text):\n",
    "    \"\"\"\n",
    "    Detect entities in text\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "\n",
    "    text: str\n",
    "        Text to be labeled\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "\n",
    "    segments: list of dicts\n",
    "        The segments.\n",
    "    \"\"\"\n",
    "    word_and_labels = label_words(text, model, tokenizer)\n",
    "    segments = bio_to_segments(word_and_labels)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokens': ['John', 'Wayne'], 'type': 'PER', 'text': 'John Wayne'},\n",
       " {'tokens': ['Uruguay,'], 'type': 'LOC', 'text': 'Uruguay,'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_entities(\"My name is John Wayne y soy el presidente de Uruguay, pedazo de cabrón\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Me', 'O'),\n",
       " ('llamo', 'O'),\n",
       " ('Juan', 'B-PER'),\n",
       " ('Pablo,', 'I-PER'),\n",
       " ('me', 'O'),\n",
       " ('gusta', 'O'),\n",
       " ('ir', 'O'),\n",
       " ('a', 'O'),\n",
       " ('25', 'B-LOC'),\n",
       " ('de', 'I-LOC'),\n",
       " ('Mayo', 'I-LOC')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_words(\"Me llamo Juan Pablo, me gusta ir a 25 de Mayo\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pysentimiento-Iym1HT9b-py3.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "296b0e3a48db82aa8dc0d9170c0bf1a3bf60424c34726914e60631e41e51dd6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

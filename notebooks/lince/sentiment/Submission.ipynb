{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robertuito-lince-pos  robertuito-lince-sentiment  robertuito-lince-sentiment-2\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../../../models/robertuito-lince-sentiment/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../../../models/robertuito-lince-sentiment\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 1,\n",
      "    \"positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30002\n",
      "}\n",
      "\n",
      "loading weights file ../../../models/robertuito-lince-sentiment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ../../../models/robertuito-lince-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "Didn't find file ../../../models/robertuito-lince-sentiment/added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ../../../models/robertuito-lince-sentiment/special_tokens_map.json\n",
      "loading file ../../../models/robertuito-lince-sentiment/tokenizer_config.json\n",
      "loading file ../../../models/robertuito-lince-sentiment/tokenizer.json\n",
      "Reusing dataset lince (/root/.cache/huggingface/datasets/lince/sa_spaeng/1.0.0/10d41747f55f0849fa84ac579ea1acfa7df49aa2015b60426bc459c111b3d589)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf31e53f82c406db7a5d27ac148ebc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/lince/sa_spaeng/1.0.0/10d41747f55f0849fa84ac579ea1acfa7df49aa2015b60426bc459c111b3d589/cache-029bff53c5a1fd47.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/lince/sa_spaeng/1.0.0/10d41747f55f0849fa84ac579ea1acfa7df49aa2015b60426bc459c111b3d589/cache-818d6ebd3452f42c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/lince/sa_spaeng/1.0.0/10d41747f55f0849fa84ac579ea1acfa7df49aa2015b60426bc459c111b3d589/cache-419ab4c207a77fce.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/lince/sa_spaeng/1.0.0/10d41747f55f0849fa84ac579ea1acfa7df49aa2015b60426bc459c111b3d589/cache-53d17b0587842a60.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/lince/sa_spaeng/1.0.0/10d41747f55f0849fa84ac579ea1acfa7df49aa2015b60426bc459c111b3d589/cache-eb8144edb12d19a4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/lince/sa_spaeng/1.0.0/10d41747f55f0849fa84ac579ea1acfa7df49aa2015b60426bc459c111b3d589/cache-e4ce36494780407e.arrow\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pysentimiento.lince.sentiment import load_datasets\n",
    "\n",
    "model_path = \"../../../models/robertuito-lince-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.model_max_length = 128\n",
    "\n",
    "_, _, test_dataset = load_datasets(lang=\"es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/lince/sa_spaeng/1.0.0/10d41747f55f0849fa84ac579ea1acfa7df49aa2015b60426bc459c111b3d589/cache-6876735a70d370be.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=False, truncation=True)\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize, batched=True, batch_size=32, \n",
    ")\n",
    "test_dataset = test_dataset.remove_columns([\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: idx, text, lid, words, sa.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4736\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='148' max='148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [148/148 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"./test/\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    do_train=False,\n",
    ")\n",
    "\n",
    "trainer_args = {\n",
    "    \"model\": model,\n",
    "    \"args\": train_args,\n",
    "    \"eval_dataset\": test_dataset,\n",
    "    \"tokenizer\": tokenizer,\n",
    "}\n",
    "\n",
    "eval_trainer = Trainer(**trainer_args)\n",
    "ret = eval_trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4736, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ret.predictions.shape[0] == len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cecbb3e2744d0798e5617c0fa631ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pysentimiento.lince.sentiment import id2label\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for pred in tqdm(ret.predictions):\n",
    "    pred = id2label[pred.argmax()]\n",
    "    outputs.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neutral': 1115, 'positive': 2941, 'negative': 680})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = \"../submissions/01_robertuito/sa_spa_eng.txt\"\n",
    "\n",
    "with open(submission_path, \"w\") as f:\n",
    "    for i, output in enumerate(outputs):\n",
    "        f.write(output + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108212755"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "sum(map(torch.numel, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "387abc9bc94d8eb1bd0148a5d4cb2bf99bc3b40fa501b808e2b508b4f65ed831"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('pysentimiento-bwlKzHxB-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

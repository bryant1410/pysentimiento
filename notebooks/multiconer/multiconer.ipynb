{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset multiconer2 (/users/jmperez/.cache/huggingface/datasets/aashsach___multiconer2/en/1.0.0/f8781e365f606105430f8dae7fad131df6b2e8cd35c0973165579f0ddc8a1bbc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95afb896670472085adeccf7ba2d6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "ds = load_dataset(\"aashsach/multiconer2\", \"en\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/aashsach___multiconer2/en/1.0.0/f8781e365f606105430f8dae7fad131df6b2e8cd35c0973165579f0ddc8a1bbc/cache-65691de3e43a9f42.arrow\n",
      "Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/aashsach___multiconer2/en/1.0.0/f8781e365f606105430f8dae7fad131df6b2e8cd35c0973165579f0ddc8a1bbc/cache-7354ddfb71e081c2.arrow\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(lambda ex: {\"text\": \" \".join(ex[\"tokens\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pysentimiento.config import config\n",
    "\n",
    "# URL for the Google Knowledge Graph Search API\n",
    "GOOGLE_KG_URL = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "\n",
    "# URL for retrieving wikidata id\n",
    "WIKIDATA_URL_TEMPLATE = \"https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&titles={}&format=json\"\n",
    "\n",
    "API_KEY = config['GOOGLE']['API_KEY']\n",
    "\n",
    "def kg_search(query, api_key=API_KEY, limit=10, indent=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Performs a Knowledge Graph Search query.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        query (str): The query string.\n",
    "\n",
    "        params (dict): The parameters to pass to the API.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        dict: The JSON response from the API.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove accents from query\n",
    "    query = query.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Merge params and kwargs\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'key': api_key,\n",
    "        'limit': limit,\n",
    "        'indent': indent,\n",
    "    }\n",
    "    params.update(kwargs)\n",
    "\n",
    "    req = requests.get(GOOGLE_KG_URL, params=params)\n",
    "    jsn = req.json()\n",
    "\n",
    "    return jsn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = ds[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(id2label)}\n",
    "label2id = {label: i for i, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/jmperez/projects/pysentimiento/pysentimiento/ner.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "from pysentimiento.ner import tokenize_and_align_labels, compute_metrics\n",
    "from pysentimiento.training import train_and_eval\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from pysentimiento.tuning import get_training_arguments\n",
    "\n",
    "base_model = \"roberta-large\"\n",
    "\n",
    "\n",
    "training_args = get_training_arguments(\n",
    "    base_model, task_name=\"multiconer\", lang=\"en\",\n",
    "    metric_for_best_model=\"eval/micro_f1\", use_defaults_if_not_tuned=True\n",
    ")\n",
    "\n",
    "training_args.num_train_epochs = 3\n",
    "training_args.per_device_train_batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds, id2label):\n",
    "    \"\"\"\n",
    "    Compute metrics for NER\n",
    "    \"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100]\n",
    "                   for label in labels]\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # I also calculate mention metrics\n",
    "    # That is, we ignore the I- and B- tags and only consider O and MENTION\n",
    "    def label_to_mention(label):\n",
    "        if label == \"O\":\n",
    "            return \"O\"\n",
    "        elif label.startswith(\"B-\"):\n",
    "            return \"B-MENTION\"\n",
    "        else:\n",
    "            return \"I-MENTION\"\n",
    "\n",
    "\n",
    "    mention_labels = [\n",
    "        [label_to_mention(l) for l in labels]\n",
    "        for labels in true_labels\n",
    "    ]\n",
    "\n",
    "    mention_predictions = [\n",
    "        [label_to_mention(p) for p in labels]\n",
    "        for labels in true_predictions\n",
    "    ]\n",
    "    \n",
    "    all_metrics = metric.compute(\n",
    "        predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    mention_metrics = metric.compute(\n",
    "        predictions=mention_predictions, references=mention_labels)\n",
    "\n",
    "    ret = {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"mention_precision\": mention_metrics[\"overall_precision\"],\n",
    "        \"mention_recall\": mention_metrics[\"overall_recall\"],\n",
    "        \"mention_f1\": mention_metrics[\"overall_f1\"],\n",
    "        \"macro_f1\": f1_score(true_labels, true_predictions, average=\"macro\"),\n",
    "        \"micro_f1\": f1_score(true_labels, true_predictions, average=\"micro\"),\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "    for k, v in all_metrics.items():\n",
    "        if not k.startswith(\"overall\"):\n",
    "            ret[k + \"_f1\"] = v[\"f1\"]\n",
    "            ret[k + \"_precision\"] = v[\"precision\"]\n",
    "            ret[k + \"_recall\"] = v[\"recall\"]\n",
    "\n",
    "    return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/aashsach___multiconer2/en/1.0.0/f8781e365f606105430f8dae7fad131df6b2e8cd35c0973165579f0ddc8a1bbc/cache-82ee47a5db6f19a2.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28df8747efac4129a1e9c1d4b93cafdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/aashsach___multiconer2/en/1.0.0/f8781e365f606105430f8dae7fad131df6b2e8cd35c0973165579f0ddc8a1bbc/cache-a0486d7f44b4d6b7.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /users/jmperez/.cache/huggingface/datasets/aashsach___multiconer2/en/1.0.0/f8781e365f606105430f8dae7fad131df6b2e8cd35c0973165579f0ddc8a1bbc/cache-a0486d7f44b4d6b7.arrow\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, id, ner_macro_tags, word_ids, text, ner_tags. If tokens, id, ner_macro_tags, word_ids, text, ner_tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/users/jmperez/projects/pysentimiento/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16778\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3147\n",
      "  Number of trainable parameters = 354381891\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjmperez\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/jmperez/projects/pysentimiento/notebooks/multiconer/wandb/run-20230131_182946-rygau7qi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jmperez/huggingface/runs/rygau7qi\" target=\"_blank\">glittering-mandu-6</a></strong> to <a href=\"https://wandb.ai/jmperez/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/jmperez/huggingface\" target=\"_blank\">https://wandb.ai/jmperez/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/jmperez/huggingface/runs/rygau7qi\" target=\"_blank\">https://wandb.ai/jmperez/huggingface/runs/rygau7qi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3147' max='3147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3147/3147 14:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Mention Precision</th>\n",
       "      <th>Mention Recall</th>\n",
       "      <th>Mention F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Aerospacemanufacturer F1</th>\n",
       "      <th>Aerospacemanufacturer Precision</th>\n",
       "      <th>Aerospacemanufacturer Recall</th>\n",
       "      <th>Anatomicalstructure F1</th>\n",
       "      <th>Anatomicalstructure Precision</th>\n",
       "      <th>Anatomicalstructure Recall</th>\n",
       "      <th>Artwork F1</th>\n",
       "      <th>Artwork Precision</th>\n",
       "      <th>Artwork Recall</th>\n",
       "      <th>Artist F1</th>\n",
       "      <th>Artist Precision</th>\n",
       "      <th>Artist Recall</th>\n",
       "      <th>Athlete F1</th>\n",
       "      <th>Athlete Precision</th>\n",
       "      <th>Athlete Recall</th>\n",
       "      <th>Carmanufacturer F1</th>\n",
       "      <th>Carmanufacturer Precision</th>\n",
       "      <th>Carmanufacturer Recall</th>\n",
       "      <th>Cleric F1</th>\n",
       "      <th>Cleric Precision</th>\n",
       "      <th>Cleric Recall</th>\n",
       "      <th>Clothing F1</th>\n",
       "      <th>Clothing Precision</th>\n",
       "      <th>Clothing Recall</th>\n",
       "      <th>Disease F1</th>\n",
       "      <th>Disease Precision</th>\n",
       "      <th>Disease Recall</th>\n",
       "      <th>Drink F1</th>\n",
       "      <th>Drink Precision</th>\n",
       "      <th>Drink Recall</th>\n",
       "      <th>Facility F1</th>\n",
       "      <th>Facility Precision</th>\n",
       "      <th>Facility Recall</th>\n",
       "      <th>Food F1</th>\n",
       "      <th>Food Precision</th>\n",
       "      <th>Food Recall</th>\n",
       "      <th>Humansettlement F1</th>\n",
       "      <th>Humansettlement Precision</th>\n",
       "      <th>Humansettlement Recall</th>\n",
       "      <th>Medicalprocedure F1</th>\n",
       "      <th>Medicalprocedure Precision</th>\n",
       "      <th>Medicalprocedure Recall</th>\n",
       "      <th>Medication/vaccine F1</th>\n",
       "      <th>Medication/vaccine Precision</th>\n",
       "      <th>Medication/vaccine Recall</th>\n",
       "      <th>Musicalgrp F1</th>\n",
       "      <th>Musicalgrp Precision</th>\n",
       "      <th>Musicalgrp Recall</th>\n",
       "      <th>Musicalwork F1</th>\n",
       "      <th>Musicalwork Precision</th>\n",
       "      <th>Musicalwork Recall</th>\n",
       "      <th>Org F1</th>\n",
       "      <th>Org Precision</th>\n",
       "      <th>Org Recall</th>\n",
       "      <th>Otherloc F1</th>\n",
       "      <th>Otherloc Precision</th>\n",
       "      <th>Otherloc Recall</th>\n",
       "      <th>Otherper F1</th>\n",
       "      <th>Otherper Precision</th>\n",
       "      <th>Otherper Recall</th>\n",
       "      <th>Otherprod F1</th>\n",
       "      <th>Otherprod Precision</th>\n",
       "      <th>Otherprod Recall</th>\n",
       "      <th>Politician F1</th>\n",
       "      <th>Politician Precision</th>\n",
       "      <th>Politician Recall</th>\n",
       "      <th>Privatecorp F1</th>\n",
       "      <th>Privatecorp Precision</th>\n",
       "      <th>Privatecorp Recall</th>\n",
       "      <th>Publiccorp F1</th>\n",
       "      <th>Publiccorp Precision</th>\n",
       "      <th>Publiccorp Recall</th>\n",
       "      <th>Scientist F1</th>\n",
       "      <th>Scientist Precision</th>\n",
       "      <th>Scientist Recall</th>\n",
       "      <th>Software F1</th>\n",
       "      <th>Software Precision</th>\n",
       "      <th>Software Recall</th>\n",
       "      <th>Sportsgrp F1</th>\n",
       "      <th>Sportsgrp Precision</th>\n",
       "      <th>Sportsgrp Recall</th>\n",
       "      <th>Sportsmanager F1</th>\n",
       "      <th>Sportsmanager Precision</th>\n",
       "      <th>Sportsmanager Recall</th>\n",
       "      <th>Station F1</th>\n",
       "      <th>Station Precision</th>\n",
       "      <th>Station Recall</th>\n",
       "      <th>Symptom F1</th>\n",
       "      <th>Symptom Precision</th>\n",
       "      <th>Symptom Recall</th>\n",
       "      <th>Vehicle F1</th>\n",
       "      <th>Vehicle Precision</th>\n",
       "      <th>Vehicle Recall</th>\n",
       "      <th>Visualwork F1</th>\n",
       "      <th>Visualwork Precision</th>\n",
       "      <th>Visualwork Recall</th>\n",
       "      <th>Writtenwork F1</th>\n",
       "      <th>Writtenwork Precision</th>\n",
       "      <th>Writtenwork Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>0.428706</td>\n",
       "      <td>0.496737</td>\n",
       "      <td>0.528549</td>\n",
       "      <td>0.758703</td>\n",
       "      <td>0.739969</td>\n",
       "      <td>0.749219</td>\n",
       "      <td>0.425935</td>\n",
       "      <td>0.512150</td>\n",
       "      <td>0.907003</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.751111</td>\n",
       "      <td>0.710084</td>\n",
       "      <td>0.797170</td>\n",
       "      <td>0.704403</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.468571</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.306220</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.323077</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.280992</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>0.314815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246900</td>\n",
       "      <td>0.254402</td>\n",
       "      <td>0.625460</td>\n",
       "      <td>0.655864</td>\n",
       "      <td>0.864093</td>\n",
       "      <td>0.863426</td>\n",
       "      <td>0.863759</td>\n",
       "      <td>0.557275</td>\n",
       "      <td>0.640301</td>\n",
       "      <td>0.927494</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.758782</td>\n",
       "      <td>0.753488</td>\n",
       "      <td>0.764151</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.819820</td>\n",
       "      <td>0.805310</td>\n",
       "      <td>0.834862</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.692913</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.578035</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.488263</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.700855</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.759259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.233026</td>\n",
       "      <td>0.669151</td>\n",
       "      <td>0.692901</td>\n",
       "      <td>0.883127</td>\n",
       "      <td>0.880401</td>\n",
       "      <td>0.881762</td>\n",
       "      <td>0.609042</td>\n",
       "      <td>0.680819</td>\n",
       "      <td>0.933949</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.805310</td>\n",
       "      <td>0.858491</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.753086</td>\n",
       "      <td>0.772152</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.846847</td>\n",
       "      <td>0.831858</td>\n",
       "      <td>0.862385</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.609195</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.485981</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.692913</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.740741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, id, ner_macro_tags, word_ids, text, ner_tags. If tokens, id, ner_macro_tags, word_ids, text, ner_tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 871\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1049\n",
      "Configuration saved in ./results/checkpoint-1049/config.json\n",
      "Model weights saved in ./results/checkpoint-1049/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1049/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1049/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, id, ner_macro_tags, word_ids, text, ner_tags. If tokens, id, ner_macro_tags, word_ids, text, ner_tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 871\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-2098\n",
      "Configuration saved in ./results/checkpoint-2098/config.json\n",
      "Model weights saved in ./results/checkpoint-2098/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2098/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2098/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, id, ner_macro_tags, word_ids, text, ner_tags. If tokens, id, ner_macro_tags, word_ids, text, ner_tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 871\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-3147\n",
      "Configuration saved in ./results/checkpoint-3147/config.json\n",
      "Model weights saved in ./results/checkpoint-3147/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3147/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3147/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-3147 (score: 0.6090423127061736).\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, id, ner_macro_tags, word_ids, text, ner_tags. If tokens, id, ner_macro_tags, word_ids, text, ner_tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 871\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<transformers.trainer.Trainer at 0x7fc711b57b20>,\n",
       " PredictionOutput(predictions=array([[[ 1.1949544e+01, -5.8727419e-01,  1.4300755e-01, ...,\n",
       "           3.9282510e-01,  9.8326951e-01, -2.9624763e-01],\n",
       "         [ 2.2967744e+00,  8.9632744e-01, -6.3836312e-01, ...,\n",
       "          -1.2158564e+00,  1.4580320e-01, -1.2456135e+00],\n",
       "         [ 2.5295413e+00, -3.5749546e-01,  1.4291887e-01, ...,\n",
       "          -2.7293724e-01, -7.1496040e-01, -1.1094067e+00],\n",
       "         ...,\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02]],\n",
       " \n",
       "        [[ 1.4874663e+01, -2.1715183e+00, -1.6531572e+00, ...,\n",
       "           1.8414363e-01,  2.0816400e+00,  7.9782331e-01],\n",
       "         [ 3.0692861e+00,  4.9591625e-01, -1.2054052e+00, ...,\n",
       "          -1.9757326e+00,  3.2628140e-01, -1.3165787e+00],\n",
       "         [ 2.1875460e+00, -1.3531382e-01, -1.2386080e+00, ...,\n",
       "          -1.4430312e+00, -5.1636195e-01, -8.5731649e-01],\n",
       "         ...,\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02]],\n",
       " \n",
       "        [[ 1.4463052e+01, -2.0808952e+00, -2.2051582e+00, ...,\n",
       "          -2.8729805e-01,  2.2726619e+00,  4.7967800e-01],\n",
       "         [ 1.4477163e+01, -1.6765990e+00, -2.2004359e+00, ...,\n",
       "          -4.2694023e-01,  2.6205246e+00,  4.9994916e-01],\n",
       "         [ 1.4462547e+01, -2.0808969e+00, -2.2066960e+00, ...,\n",
       "          -2.8663695e-01,  2.2724278e+00,  4.8047864e-01],\n",
       "         ...,\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 1.4890205e+01, -2.2413404e+00, -1.7239074e+00, ...,\n",
       "           6.1136717e-01,  1.6610090e+00,  3.4946278e-01],\n",
       "         [ 1.5433097e+01, -1.8692589e+00, -1.7875086e+00, ...,\n",
       "           9.6045339e-01,  1.8966672e+00,  8.8314539e-01],\n",
       "         [ 1.4966437e+01, -2.4035771e+00, -1.8974038e+00, ...,\n",
       "           1.1072389e+00,  3.6690969e+00,  1.1604235e+00],\n",
       "         ...,\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02]],\n",
       " \n",
       "        [[ 1.4395838e+01, -8.4153950e-01, -8.3995706e-01, ...,\n",
       "           1.3268401e-01,  7.5951797e-01, -1.2363561e-01],\n",
       "         [ 1.3377832e+01,  2.3118891e-01, -1.4711504e+00, ...,\n",
       "          -6.0922927e-01,  1.8043582e+00, -9.6570992e-01],\n",
       "         [ 1.3985009e+01, -1.3440592e-02, -1.0513804e+00, ...,\n",
       "           9.1127500e-02,  1.3083924e+00,  8.0401160e-02],\n",
       "         ...,\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02]],\n",
       " \n",
       "        [[ 1.3793729e+01, -2.2273333e+00, -3.1115193e+00, ...,\n",
       "           2.3593950e+00,  2.5321391e+00,  1.9055349e+00],\n",
       "         [ 1.1558028e+01, -2.8776735e-01, -2.2632322e+00, ...,\n",
       "          -2.9431421e-01,  3.3816531e+00,  1.9032133e-01],\n",
       "         [ 4.1934614e+00,  7.8925914e-01, -1.7401322e+00, ...,\n",
       "           1.5235510e-01,  2.7567852e+00,  1.1525694e+00],\n",
       "         ...,\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02],\n",
       "         [-1.0000000e+02, -1.0000000e+02, -1.0000000e+02, ...,\n",
       "          -1.0000000e+02, -1.0000000e+02, -1.0000000e+02]]], dtype=float32), label_ids=array([[-100,   39, -100, ..., -100, -100, -100],\n",
       "        [-100,   39, -100, ..., -100, -100, -100],\n",
       "        [-100,    0,    0, ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100,    0,    0, ..., -100, -100, -100],\n",
       "        [-100,    0, -100, ..., -100, -100, -100],\n",
       "        [-100,    0,   45, ..., -100, -100, -100]]), metrics={'test_loss': 0.2330256700515747, 'test_precision': 0.669150521609538, 'test_recall': 0.6929012345679012, 'test_mention_precision': 0.8831269349845201, 'test_mention_recall': 0.8804012345679012, 'test_mention_f1': 0.8817619783616691, 'test_macro_f1': 0.6090423127061736, 'test_micro_f1': 0.6808188021228203, 'test_accuracy': 0.9339488103280043, 'test_AerospaceManufacturer_f1': 0.5217391304347826, 'test_AerospaceManufacturer_precision': 0.46153846153846156, 'test_AerospaceManufacturer_recall': 0.6, 'test_AnatomicalStructure_f1': 0.6666666666666667, 'test_AnatomicalStructure_precision': 0.6875, 'test_AnatomicalStructure_recall': 0.6470588235294118, 'test_ArtWork_f1': 0.380952380952381, 'test_ArtWork_precision': 0.5, 'test_ArtWork_recall': 0.3076923076923077, 'test_Artist_f1': 0.8310502283105022, 'test_Artist_precision': 0.8053097345132744, 'test_Artist_recall': 0.8584905660377359, 'test_Athlete_f1': 0.7625, 'test_Athlete_precision': 0.7530864197530864, 'test_Athlete_recall': 0.7721518987341772, 'test_CarManufacturer_f1': 0.7500000000000001, 'test_CarManufacturer_precision': 0.8181818181818182, 'test_CarManufacturer_recall': 0.6923076923076923, 'test_Cleric_f1': 0.4166666666666667, 'test_Cleric_precision': 0.5555555555555556, 'test_Cleric_recall': 0.3333333333333333, 'test_Clothing_f1': 0.6, 'test_Clothing_precision': 0.6, 'test_Clothing_recall': 0.6, 'test_Disease_f1': 0.5142857142857143, 'test_Disease_precision': 0.5294117647058824, 'test_Disease_recall': 0.5, 'test_Drink_f1': 0.631578947368421, 'test_Drink_precision': 0.75, 'test_Drink_recall': 0.5454545454545454, 'test_Facility_f1': 0.7047619047619047, 'test_Facility_precision': 0.6981132075471698, 'test_Facility_recall': 0.7115384615384616, 'test_Food_f1': 0.43243243243243246, 'test_Food_precision': 0.4444444444444444, 'test_Food_recall': 0.42105263157894735, 'test_HumanSettlement_f1': 0.8468468468468467, 'test_HumanSettlement_precision': 0.831858407079646, 'test_HumanSettlement_recall': 0.8623853211009175, 'test_MedicalProcedure_f1': 0.64, 'test_MedicalProcedure_precision': 0.6666666666666666, 'test_MedicalProcedure_recall': 0.6153846153846154, 'test_Medication/Vaccine_f1': 0.7368421052631577, 'test_Medication/Vaccine_precision': 0.7, 'test_Medication/Vaccine_recall': 0.7777777777777778, 'test_MusicalGRP_f1': 0.7733333333333334, 'test_MusicalGRP_precision': 0.7631578947368421, 'test_MusicalGRP_recall': 0.7837837837837838, 'test_MusicalWork_f1': 0.7301587301587301, 'test_MusicalWork_precision': 0.7076923076923077, 'test_MusicalWork_recall': 0.7540983606557377, 'test_ORG_f1': 0.6091954022988507, 'test_ORG_precision': 0.5520833333333334, 'test_ORG_recall': 0.6794871794871795, 'test_OtherLOC_f1': 0.48484848484848486, 'test_OtherLOC_precision': 0.47058823529411764, 'test_OtherLOC_recall': 0.5, 'test_OtherPER_f1': 0.5, 'test_OtherPER_precision': 0.4752475247524752, 'test_OtherPER_recall': 0.5274725274725275, 'test_OtherPROD_f1': 0.5591397849462366, 'test_OtherPROD_precision': 0.5909090909090909, 'test_OtherPROD_recall': 0.5306122448979592, 'test_Politician_f1': 0.48598130841121495, 'test_Politician_precision': 0.48148148148148145, 'test_Politician_recall': 0.49056603773584906, 'test_PrivateCorp_f1': 0.13333333333333333, 'test_PrivateCorp_precision': 0.25, 'test_PrivateCorp_recall': 0.09090909090909091, 'test_PublicCorp_f1': 0.5185185185185186, 'test_PublicCorp_precision': 0.5384615384615384, 'test_PublicCorp_recall': 0.5, 'test_Scientist_f1': 0.4000000000000001, 'test_Scientist_precision': 0.4, 'test_Scientist_recall': 0.4, 'test_Software_f1': 0.5714285714285715, 'test_Software_precision': 0.5333333333333333, 'test_Software_recall': 0.6153846153846154, 'test_SportsGRP_f1': 0.8275862068965518, 'test_SportsGRP_precision': 0.782608695652174, 'test_SportsGRP_recall': 0.8780487804878049, 'test_SportsManager_f1': 0.7096774193548386, 'test_SportsManager_precision': 0.7333333333333333, 'test_SportsManager_recall': 0.6875, 'test_Station_f1': 0.85, 'test_Station_precision': 0.85, 'test_Station_recall': 0.85, 'test_Symptom_f1': 0.5714285714285713, 'test_Symptom_precision': 0.5454545454545454, 'test_Symptom_recall': 0.6, 'test_Vehicle_f1': 0.5238095238095238, 'test_Vehicle_precision': 0.5, 'test_Vehicle_recall': 0.55, 'test_VisualWork_f1': 0.6929133858267716, 'test_VisualWork_precision': 0.6666666666666666, 'test_VisualWork_recall': 0.7213114754098361, 'test_WrittenWork_f1': 0.7207207207207207, 'test_WrittenWork_precision': 0.7017543859649122, 'test_WrittenWork_recall': 0.7407407407407407, 'test_runtime': 4.2376, 'test_samples_per_second': 205.542, 'test_steps_per_second': 6.608}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds[\"dev\"] = ds[\"validation\"]\n",
    "ds[\"test\"] = ds[\"validation\"]\n",
    "\n",
    "\n",
    "train_and_eval(\n",
    "    base_model, dataset=ds,\n",
    "    id2label=id2label, lang=\"en\", \n",
    "    training_args=training_args,\n",
    "    # Custom stuff for this thing to work\n",
    "    tokenize_fun=tokenize_and_align_labels,\n",
    "    auto_class=AutoModelForTokenClassification,\n",
    "    data_collator_class=DataCollatorForTokenClassification,\n",
    "    metrics_fun=lambda x: compute_metrics(x, id2label),\n",
    "    metric_for_best_model=\"micro_f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc79ed69e4c2b5a1db8fa17ebb1e82d66421519e5b018d314116a7b4cda9238"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
